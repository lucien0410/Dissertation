\chapter{Tying Up Some Loose Ends}
\label{chap:Tying_Up}

The results of the experiment in the previous chapter show that the Parallel-Partial treatment improves the baseline system (system that trained on Gaelic-to-English parallel data only) tremendously. As such, the hypothesis that gloss information helps Machine Translation is supported.

However, there are loose ends that need to be tied up. First, the effectiveness of the Parallel-Partial treatment is demonstrated when compared with other internal treatments. We would like to know how well it performs when compared to other well-established Machine Translation systems.       
Additionally, there are some potential confounding factors that may jeopardize the validity of the experiment results. 

To tie up these loose ends, this chapter reports additional experiments and results.

\section{Comparison with Google Translation}
The Parallel-Partial treatment yields superb systems compared to the baseline systems, but is it still good when we compare it to other well-established Machine Translation systems? 
To answer this question, I compare our systems with Google translation.
Specifically, I used a free Google translation API \citep{google_api} to translate the Gaelic sentences in our test set. 
Then I calculated the BLEU scores of Google's predicted outputs with the target sequences of our test set as the gold standard. In this manner, Google translation is just like an additional treatment in the comparison. The result is shown in the following table:

\SweaveInput{google_table.Rnw}

It turns out the systems trained with the Parallel-Partial treatment also vastly outperform Google translation.   

\section{Oversampling}
A potential confounding factor of the effectiveness of the Parallel-Partial treatment is that this treatment has more English sentences than the baseline systems. 

Recall what the training data for both treatments are: 

\begin{exe}
	\ex Mappings Learned in the Baseline treatment:
	\begin{xlist}
		\ex Gaelic sentences $\rightarrow$ English sentences
	\end{xlist}	
	\ex Mappings Learned in the ParaPart treatment:
	\begin{xlist}
		\ex \label{over1} Gaelic sentences $\rightarrow$ English sentences
		\ex \label{over2} Gloss lines $\rightarrow$ English sentences
		\ex Gloss lines $\rightarrow$ Gaelic sentences
		\ex Gaelic words $\rightarrow$ Gloss items
	\end{xlist}	
\end{exe}    

Specifically, because the Parallel-Partial treatment learns the mappings in (\ref{over1}) and (\ref{over2}), the English sentences are oversampled. Oversampling of the target sequences has a positive effect, because the decoding process of the Neural Machine Translation system uses the language model information of the target language. Now given that in the ParaPart treatment the quantity of English sentences (i.e. the target language) is doubled, the effectiveness of the ParaPart treatment may be just a result of this oversampling.   

To exclude this confounding factor, I oversampled the mappings in the baseline systems by repeating the mappings, so that we can have a fair comparison. Specifically, I doubled, tripled, and quadrupled the training data of the baseline treatment. The following table shows results of the oversampled baseline systems. 

\SweaveInput{over_table.Rnw}

It turns out that oversampling also improves the baseline systems; however, it is still not as effective as the Parallel-Partial treatment. Given these results, the oversampling confound is ruled out.

\section{Other Hyper-Parameters}
Another possible confounding factor is that maybe the set of the arbitrarily chosen hyper-parameters used in the experiments in chapter \ref{chap:cake2} just accidentally favors the Parallel-Partial treatment and disfavors the baseline treatment. To exclude the potential confounding effect of the hyper-parameters, I sampled some hyper-parameters by changing the size of word embedding (into $\{100, 200, 300, 400, 500\}$) and the size of the mini-batches (into $\{16, 32, 64, 128\}$) using the same training, validation, and test data used in round 0. The advantage of the Parallel-Partial treatment is consistent across the different settings of the hyper-parameters. The results are shown in the following table:  

\SweaveInput{HyPara_table.Rnw}

In the table, we can see that the sizes of word embedding and mini-batches do have an effect on the performance of the trained system. However, the Parallel-Partial treatment still outperforms the baseline no matter what the hyper-parameters are. 

\section{Interlinear Glossed Text Data In Other Languages: the Universality}
Given the above results, it is properly defended that the gloss information of Scottish Gaelic can improve Scottish Gaelic to English Neural Machine Translation systems. An interesting confounding factor of the gloss-help-hypothesis is that maybe it only works on Gaelic. Maybe the Gaelic language just has some special property that makes the gloss information relevant. Now the question is:

\begin{exe}
\ex Does the gloss-help-hypothesis hold universally across different languages?   
\end{exe}  

This section reports the experiment result using the same settings as in chapter \ref{chap:cake2} except for the fact that the source language is not Gaelic. 

The Online Database of Interlinear Text \citep{ODIN, Xia2016} is a perfect database to run this experiment. Specifically, this database extracted the interlinear glossed texts in published linguistic papers that are open on the Internet. The interlinear glossed texts are stored in a consistent and machine readable format, called xigt (xml for interlinear glossed text). The Online Database of Interlinear Text contains interlinear glossed texts in 1,495 different languages; however, only 18 languages have more than 1,000 chunks of interlinear glossed text (i.e. the 3-tuples of a source sentence, a gloss line, and an English translation). In the current translation experiment, I use the Online Database of Interlinear Text in these 18 languages.   

The result is shown in the following table:   

\SweaveInput{Other_language_table.Rnw}

\newpage
The paired t-test shows that the Parallel-Partial treatment outperforms the baseline systems 
(M\textsubscript{Baseline}=\Sexpr{m_lg}, SD\textsubscript{Baseline}=\Sexpr{sd_lg}; M\textsubscript{ParaPart}=\Sexpr{m_ParaPart}, SD\textsubscript{ParaPart}=\Sexpr{sd_ParaPart},; t(17)=\Sexpr{lg_t_val}, p < 0.01) and the GLOSS treatment 
(M\textsubscript{GLOSS}=\Sexpr{m_gloss}, SD\textsubscript{GLOSS}=\Sexpr{sd_gloss}; M\textsubscript{ParaPart}=\Sexpr{m_ParaPart}, SD\textsubscript{ParaPart}=\Sexpr{sd_ParaPart},; t(17)=\Sexpr{gl_t_val}, p < 0.01).


The result shows that incorporating gloss information works effectively across many different languages. Note that for Greek and Italian the ParaPart treatment has no effect. I do not have a well-supported explanation for now, and also it may be true that gloss information may help different languages to different degrees. In other words, for some languages gloss information is a huge booster while for other languages it is a small booster. The relation between the property of a language and how much gloss information helps that language is left for my future research.     

\section{Conclusion}

This chapter describes additional experiments and results to confirm the effectiveness of gloss information by excluding other potential confounding factors.
Given all these results, we may conclude that the gloss information is an effective booster for Neural Machine Translation systems across different hyper-parameters and languages.       

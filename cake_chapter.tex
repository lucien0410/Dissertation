\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage{gb4e}
\noautomath
\usepackage{natbib}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{python}

%% Sets page size and margins
%\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{fixltx2e}
\usepackage{Sweave}

\newcommand*{\myfont}{\fontfamily{ccr}\selectfont}
%the newcommand change the selected word into 'ccr' font
%e.g. \begin{myfont} multi-bleu.perl \end{myfont}

\title{Chapter N: Experimenting Interlinear Glossing Text}
\author{Yuan-Lu Chen}

\begin{document}
\input{cake_chapter-concordance}

\maketitle


(

\textbf{Assuming that in the previous chapters the following points are addressed already:} 
\begin{itemize}
\item The nature of glosses has been well-explained  (Target audience: CS people without any formal linguistics background):
	\begin{itemize}
	\item What glosses are: A basic intro of interlinear gloss for non-linguists
    \item The golden nature of glosses (encodes NON-LINEAR syntax (i.e. structure parse) and semantics information) 
    \item The potential of gloss:	
		\begin{itemize}
		\item potential: providing disambiguation, labeling important grammar morphemes in the source language, providing morphological analysis, providing one-to-many and many-to-one relations of source tokens and target tokens.  
		\end{itemize}
	\end{itemize}
\item A history of machine translation, and a non-mathy description of the methods of doing machine translation. (Target reader: theoretical linguists)
\end{itemize}


)


\section{Introduction}
The Innovation is to incorporate the gloss information of Interlinear Glossed Text data into machine translation. 

In supervised machine learning models, two factors effects the performance of the trained systems \citep{kotsiantis2007supervised}: a.) the quality of the training data and b.) the choices of the features. The properties of the gloss data as described in *CHAPTERXYZ* make it a better training data than natural language data (Scottish Gaelic in the current case) for the following reasons. First, glosses are more purified that natural language words. The most ideal meaning representation system should be built with mappings where one meaning or function is mapped to one and only one representation. Natural languages fail to do so, given that synonyms and ambiguous words/phrases are ubiquitous in natural languages. On the other hand, Glosses provide this one-to-one mapping. Second, the gloss data provides hierarchical (non-linear) syntactic parsing information to some degree. To determine what the gloss of a word is, linguists have to look for hierarchical context information. 

Therefore, theoretically incorporation of the gloss data should improve the translation systems. Specifically, I propose the following hypothesis:
\begin{exe}  
\ex \textbf{Gloss-helps-hypothesis: the translation systems trained with the gloss data incorporated should outperform the systems trained with only Gaelic and English sentences pairs (i.e. without gloss data).}

The hypothesis can have two versions: strong and weak:
	\begin{xlist}
	\ex \label{strong_hy} Strong version: Gloss may replace the source natural language totally, and the system outperforms the system trained with source natural language to target language sentence pairs (i.e. the baseline systems).   
	\ex \label{weak_hy} Weak version: Gloss only increases the performance of the baseline systems, but cannot replace the source language. 
	\end{xlist}
\end{exe}

The experiments reveal that replacing Gaelic words with glosses doesn't boost up the performance of the translation systems. Thus, the strong version (replacing-Gaelic-with gloss) of the gloss-helps-hypothesis is not attested. However,it is found that if the Gaelic data and the gloss data are combined in a specific way as the training data, the performance of the systems is improved significantly.  

This chapter describes the experiments conducted to test the gloss-helps-hypothesis and the results attest the weak version. 
The rest of the chapter is organized as follows: Section \ref{sec:experimet_setting} describes the constant parameter settings across all the experiments, section \ref{gd_to_gl_to_en} tests the hypothesis in (\ref{strong_hy}), section \ref{gd_plus_gl_to_en} tests the hypothesis in (\ref{weak_hy}),and section 5 concludes the chapter.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Technical Settings of the Machine Translation Experiments}\label{sec:experimet_setting}
The experiments are conduced by using OpenNMT \citep{2017opennmt}, which implements the state-of-the-art neural net machine translation algorithms \citep{cho2014properties, cho2014learning, bahdanau2014neural}. 
The following default parameters settings of OpenNMT are used across all models so that the only independent variable is the type of the training data: 
\begin{itemize}
\item Word vector size: 500
\item Type of recurrent cell: Long Short Term Memory
\item Number of recurrent layers of the encoder and decoder: 2
\item Number of epochs: 13 
\item Size of mini batches: 64 
\end{itemize}
The data and the scripts will be accessible on GitHub\footnote{\url{https://github.com/lucien0410/Scottish_Gaelic}}, so that the results can be reproduced.   

\section{Gloss Representation Solely Does NOT Outperform Gaelic Sentences} \label{gd_to_gl_to_en}
This section tests the strong version of Gloss-helps-hypothesis in (\ref{strong_hy}).
Given the assumption that gloss may be better than any natural language in terms of representing meanings, it is expected that for neural net machine translation systems it is easier to learn how to translate from the glosses of Scottish Gaelic to English than to learn how to translate from Scottish Gaelic to English. However, the results show that there is no significance difference between the two types of data (i.e. GLOSS $\rightarrow$ English and Gaelic $\rightarrow$ English). 

\subsection{Procedure of the Experiments}
I use repeated random sub-sampling validation to compare the performances of the two type of models. 
Specifically, the samples (i.e. pairs of a gloss line and an English sentence, or pairs of a Gaelic sentence and an English sentence) are randomly split into three datasets: training set (N=6,388), validation set (N=1,000), and test set (N=1,000). The model is trained with the training and validation set (i.e. the model learns how to map the source sequence to the target sequence); the trained model then maps the source sequences of the test set to the predicted target sequences. To evaluate the model, the predicted target sequences are checked against the target sequences of the test set (i.e. the gold standard). Specifically, the BLEU (bilingual evaluation understudy)\footnote{There are other automatic machine translation evaluation algorithms available, such as translation edit rate \citep{Ter} and Damerauâ€“Levenshtein distance \citep{damerau1964technique, levenshtein1966binary}. BLEU is chosen for the current experiments because it is the most widely used evaluation algorithm, and the correlation between the BLUE score evaluation and human judgment evaluation is also well-acknowledged.} score metric \citep{bleu} of each prediction is calculated using the \begin{myfont} multi-bleu.perl\end{myfont}\footnote{The script can be downloaded from: \url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} 
script, a public implementation of Moses \citep{moses}. The BLEU score calculation is an automatic evaluation of how similar two copora are. In the current experiments we are comparing the predicted target sequences and the gold standard. The BLEU score of 100 means the two copora are identical, and the BLEU score of 0 means the two copora are completely distinct from each other.
This procedure is executed for ten times. 

The procedure is depicted schematically as follows:
\begin{exe}
\ex Definitions of datasets:
	\begin{xlist}
	\ex Let: \\
    		Train, Validation, and Test be sets of random indexes from 0 to 8,387, and \\
    		Train $\cap$ Validation $\cap$ Test = $\emptyset$ 
	\ex Gloss to English
		\begin{xlist}
		\ex \label{GLOSStoENTrain} GLOSStoEN\textsubscript{Train}   = $\{<gloss_i,En_i>  \mid i \in Train \}$ \\
		\ex \label{GLOSStoENVal} GLOSStoEN\textsubscript{Validation}   = $\{<gloss_i,En_i>  \mid i \in Validation \}$ \\
		\ex \label{GLOSStoENTest}GLOSStoEN\textsubscript{Test} = $\{<gloss_i,En_i>  \mid i \in Test \}$ \\
		\end{xlist}
	
	\ex Gaelic to English
		\begin{xlist}
		\ex \label{GDtoENTrain} GDtoEN\textsubscript{Train}   = $\{<GD_i,En_i>  \mid i \in Train \}$ \\
		\ex \label{GDtoENVal} GDtoEN\textsubscript{Validation}   = $\{<GD_i,En_i>  \mid i \in Validation \}$ \\
		\ex \label{GDtoENTest} GDtoEN\textsubscript{Test}    = $\{<GD_i,En_i>  \mid i \in Test \}$ \\
		\end{xlist}
	\end{xlist}
\end{exe}


\begin{exe}
\ex Models and Predictions: 
	\begin{xlist}
	\ex Model\textsubscript{GLOSStoEN} = Model trained with (\ref{GLOSStoENTrain}) and (\ref{GLOSStoENVal})
	\ex Predictions\textsubscript{GLOSStoEN} = A list of English sequences that Model\textsubscript{GLOSStoEN} maps to from the gloss sequences in (\ref{GLOSStoENTest}) 
	\ex Model\textsubscript{GDtoEN} = Model trained with (\ref{GDtoENTrain}) and (\ref{GDtoENVal}) 
	\ex Predictions\textsubscript{GDtoEN} = A list of English sequences that Model\textsubscript{GDtoEN} maps to from the gloss sequences in (\ref{GDtoENTest}) 
	\end{xlist}	
\ex Gold-Standard = English sequences in (\ref{GLOSStoENTest}) = English sequences in (\ref{GDtoENTest})
\ex Scores: \\
  \begin{xlist}
	\ex Score\textsubscript{GLOSStoEN} = BLEU(Gold-Standard, Predictions\textsubscript{GLOSStoEN}) \\
	\ex Score\textsubscript{GDtoEN} = BLEU(Gold-Standard, Predictions\textsubscript{GDtoEN}) \\
  \end{xlist}
\end{exe}


\subsection{Result} \label{gdglen_results}
After ten rounds of repeated random sub-sampling validation, ten pairs of scores of the two models are generated, as shown in the following table. 


% latex table generated in R 3.4.4 by xtable 1.8-2 package
% Sat Mar 17 22:19:52 2018
\begin{table}[ht]
\centering
\begin{tabular}{lrr}
  \hline
Round & GLOSS & Gaelic \\ 
  \hline
1 & 18.39 & 17.29 \\ 
  2 & 18.00 & 16.42 \\ 
  3 & 16.02 & 15.29 \\ 
  4 & 20.22 & 15.97 \\ 
  5 & 19.02 & 17.79 \\ 
  6 & 15.53 & 16.73 \\ 
  7 & 18.00 & 17.11 \\ 
  8 & 20.08 & 16.37 \\ 
  9 & 15.82 & 15.93 \\ 
  10 & 15.93 & 16.99 \\ 
   \hline
Mean & 17.70 & 16.59 \\ 
   \hline
\end{tabular}
\caption{BLEU scores of Model\textsubscript{GLOSStoEN} and Model\textsubscript{GDtoEN}} 
\label{Table:BLEUGlossGD}
\end{table}
It is not attested that the models trained with gloss data (M=17.70, SD=1.78) outperform the models trained with Gaelic (M=16.59, SD=0.74); t(9)=1.97, p=0.08.

\subsection{Discussion}
If we assume that the performances of the MT models are correlated with the quality of the representation of meanings in the source sequences, then given the results in (\ref{gdglen_results}) show that glosses and natural languages are about the same in terms of representing meanings. 
Now that the weak version of the Gloss-helps-hypothesis fails; we may now try to incorporate Gaelic and Gloss sentences in to training data. The experiments and results are reported in the next section. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Section: Combining Gaelic words with Glosses }\label{gd_plus_gl_to_en}

% latex table generated in R 3.4.4 by xtable 1.8-2 package
% Sat Mar 17 22:19:52 2018
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
Round & GLOSS & Gaelic & ConcatGLOSSGaelic & GLOSS.Gaelic & hyGD & hyGW \\ 
  \hline
0 & 18.39 & 17.29 & 15.42 & 13.67 & 9.44 & 15.95 \\ 
  1 & 18.00 & 16.42 & 14.31 & 12.49 & 9.07 & 15.60 \\ 
  2 & 16.02 & 15.29 & 15.38 & 11.01 & 7.69 & 14.15 \\ 
  3 & 20.22 & 15.97 & 14.18 & 12.33 & 9.12 & 14.72 \\ 
  4 & 19.02 & 17.79 & 18.63 & 12.56 & 9.08 & 15.74 \\ 
  5 & 15.53 & 16.73 & 14.89 & 12.13 & 10.45 & 14.88 \\ 
  6 & 18.00 & 17.11 & 15.16 & 11.55 & 8.62 & 14.45 \\ 
  7 & 20.08 & 16.37 & 15.20 & 12.78 & 10.00 & 16.41 \\ 
  8 & 15.82 & 15.93 & 15.50 & 12.43 & 10.52 & 15.15 \\ 
  9 & 15.93 & 16.99 & 15.72 & 11.65 & 8.46 & 17.61 \\ 
   \hline
Mean & 17.70 & 16.59 & 15.44 & 12.26 & 9.24 & 10.24 \\ 
   \hline
\end{tabular}
\caption{More BLEU scores of the Models, where `ConcatGLOSSGaelic' is `GLOSS\textunderscore Gaelic', `GLOSS.Gaelic' is `GLOSS Gaelic', `hyGD' is using the less ambigious one from either Gaelic word or gloss with Gaelic word as the default, `hyGW' is the same as `hyGD' but with the default as gloss.} 
\label{Table:BLEUGlossGDALL}
\end{table}% latex table generated in R 3.4.4 by xtable 1.8-2 package
% Sat Mar 17 22:19:52 2018
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
Round & Para & ParaPart & ParaPartHalf & ParaPartHalfOver & Over.Gaelic & google \\ 
  \hline
0 & 25.42 & 32.64 & 24.07 & 26.31 & 29.05 & 22.09 \\ 
  1 & 25.32 & 32.28 & 18.58 & 24.85 & 28.61 & 25.38 \\ 
  2 & 20.72 & 29.94 & 18.00 & 22.96 & 23.78 & 23.72 \\ 
  3 & 22.22 & 31.18 & 22.25 & 25.48 & 27.50 & 23.21 \\ 
  4 & 24.27 & 32.83 & 23.79 & 25.33 & 25.51 & 22.31 \\ 
  5 & 24.55 & 31.11 & 21.40 & 24.39 & 27.88 & 23.41 \\ 
  6 & 27.03 & 32.19 & 23.61 & 26.29 & 25.72 & 24.53 \\ 
  7 & 25.34 & 33.52 & 23.73 & 25.61 & 27.12 & 22.78 \\ 
  8 & 24.24 & 30.93 & 23.16 & 25.59 & 25.20 & 25.67 \\ 
  9 & 25.96 & 34.35 & 24.49 & 26.32 & 26.39 & 23.42 \\ 
   \hline
Mean & 24.51 & 32.10 & 22.31 & 25.31 & 26.68 & 23.65 \\ 
   \hline
\end{tabular}
\caption{`Para' is `GLOSS -> Gaelic; Gloss -> En; Gaelic -> En', `ParaPart' is `Para plus Gaelic word token -> Gloss token', `Over' means oversampling, `Half' means using half of the training data.} 
\label{Table:BLEUGlossGDALL}
\end{table}

\bibliographystyle{te}
\bibliography{ref}


\end{document}

\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage{gb4e}
\noautomath
\usepackage{natbib}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
%\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{fixltx2e}
\usepackage{Sweave}

\newcommand*{\myfont}{\fontfamily{ccr}\selectfont}
%the newcommand change the selected word into 'ccr' font
%e.g. \begin{myfont} multi-bleu.perl \end{myfont}

\title{Building Neural Net Machine Translation Systems Using Interlinear Glossed Texts}

\date{}

\begin{document}
\maketitle
\input{short_version-concordance}

\begin{abstract}
  The gloss data that are widely used in theoretical linguistics are hidden treasures for machine translation. 
  The current paper introduces the gloss data to the natural language processing world and demonstrates a practical and effective way to incorporate gloss data into the training data for training neural net machine translation systems. 
\end{abstract}

\section{Introduction}
Interlinear Glossed Text (IGT) is widely used in linguistic studies. (1)  is an example of Scottish Gaelic IGT.
\begin{exe}  
\ex \gll    Tha a athair nas sine na a mh\`athair.\\  
            be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother
\\  
    \glt    `His father is older than his mother.'  
\end{exe}

In a simple form of IGT, the first line is a sentence of the language of interest, the second line is a word-by-word translation, annotated with relevant grammatical information, and the third line is an English translation (see \citet{bickel2008leipzig} for the complete formats and options of IGT).  

The Innovation is to incorporate the gloss information of Interlinear Glossed Text data into neural net machine translation systems.

The properties of the gloss data make it a better training data than natural language data (Scottish Gaelic in the current case) for the following reasons: 1) gloss representations cluster words with different forms into a single representation; 2) gloss representations reserve the semantic difference between homographs; 3) gloss representations are sensitive to hierarchical structures (sentence parsing). 

Consider the definite article in the following Gaelic examples. 


\begin{exe}  
\ex 
\gll tha mi a' sireadh \textbf{an} leabhair bhig ghuirm.\\
be-PRES-IND 1S PROG searching-VN \textbf{ART} book-G small-G blue-G\\
\glt `I am looking for the small blue book.' \citep[p. 29]{lamb2001scottish}

\ex 
\gll \textbf{am} fear m\`or.\\
\textbf{ART} man big\\
\glt `a big man.' \citep[p. 31]{lamb2001scottish}

\ex
\gll thuit \textbf{a'} chlach air cas mo mhn\`a.\\
fall-PAST \textbf{ART} stone on foot 1S-POSS wife-G\\
\glt`the stone fell on my wife's foot.' \citep[p. 30]{lamb2001scottish} 	

\ex
\gll doras \textbf{na} sgoile(adh). \\
door-N \textbf{ART} school-G \\
\glt `the door of the school.' \citep[p. 29]{lamb2001scottish} 	

\ex 
\gll a chuir air d\`oigh \textbf{nan} \`airidhean a-muigh a rubh' Eubhal agus an oidhche seo. \\
to put-INF on order \textbf{ART} sheilings out-LOC to point Eaval and ART night this \\
\glt `the girls big house.' \citep[p. 100]{lamb2001scottish} 

\ex
\gll f\`eis \textbf{nam} b\`ard.\\
festival \textbf{ART} poet.PL.GEN\\
\glt `festival of the poets.' \citep[p. 107]{lamb2001scottish}

\end{exe}

The definite article in Scottish Gaelic may be realized as the following forms: as \textit{an}, \textit{am}, \textit{a'}, \textit{na}, \textit{nan} or \textit{nam}. The alternation is determined by the case, gender and number of noun phrase that it modifies, and additionally phonological property of the word following it also changes the form of the definite article \citep{lamb2001scottish}. All these different realizations refer to the same concept, the definite article. The gloss representation nicely clusters them together as \textit{ART}. Learning the general distribution of the article and all its different forms is a challenge for the MT system, but the glossing information should make this easier.

Also the glosses distinguish different concepts with the form. Consider the word \textit{a'} in the following examples.  

\begin{exe}  
\ex \label{a_prog}
\gll tha mi \textbf{a'} sireadh an leabhair bhig ghuirm.\\
be-PRES-IND 1S \textbf{PROG} searching-VN ART book-G small-G blue-G\\
\glt `I am looking for the small blue book.' \citep[p. 29]{lamb2001scottish}

\ex \label{a_det}
\gll thuit \textbf{a'} chlach air cas mo mhn\`a.\\
fall-PAST \textbf{ART} stone on foot 1S-POSS wife-G\\
\glt`the stone fell on my wife's foot.' \citep[p. 30]{lamb2001scottish} 	
\end{exe}

Critically \textit{a'} in (\ref{a_prog}) is a progressive aspect marker while in (\ref{a_det}) the some form denotes to definite article. Again, the semantic difference is preserved in the gloss representations but not in natural language words.  
The gloss data also provides hierarchical (non-linear) syntactic parsing information. Consider the Gaelic word, \textit{a'}, in the above examples again, the gloss of which is decided by the hierarchical structure (i.e. constituency) of the sentences instead of the linear order of the words.    

In short, glosses are more purified and transparent than natural language words in terms of representing meanings. Therefore, theoretically the incorporation of the gloss data should improve the translation systems. Specifically, we propose the following hypothesis:

\begin{exe} 
\ex \textbf{Gloss-helps hypothesis: the translation systems trained with the gloss data incorporated should outperform the systems trained with only Gaelic and English sentences pairs (i.e. without gloss data).}\label{hypothesis}

The hypothesis can have two versions, strong and weak:
	\begin{xlist}
	\ex \label{strong_hy} Strong version: Gloss may replace the source natural language totally, and the system outperforms the system trained with source natural language to target language sentence pairs (i.e. the baseline systems).  
	\ex \label{weak_hy} Weak version: Gloss only increases the performance of the baseline systems, but cannot replace the source language.
	\end{xlist}
\end{exe}

The experiments reveal that replacing Gaelic words with glosses doesn't improve the performance of the translation systems. Thus, the strong version (replacing-Gaelic-with gloss) of the Gloss-helps hypothesis is not attested. However, it is found that if the Gaelic data and the gloss data are combined in a specific way as the training data, we term which as Parallel-Partial treatment (see section \ref{sec:ParaPart}), the performance of the systems is improved significantly. 

The rest of the paper is organized as follows: Section 2 discusses relevant literature, Section \ref{sec:experimet_setting} describes the constant parameter settings across all the experiments, section \ref{gd_to_gl_to_en} tests the hypothesis in (\ref{strong_hy}), section \ref{gd_plus_gl_to_en} tests the hypothesis in (\ref{weak_hy}) and exemplifies an effective way of incorporating the gloss information, section 6 reports other possible ways of blending glosses and Gaelic sentences, and section 7 is the conclusion. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{relate_work}
Attempts to improve machine translation systems by incorporating explicit linguistic information are reported in the literature. Syntax information is known to be effective in improving statistical machine translation (SMT). The efforts of using syntax information even derive a special type of SMT, termed as syntax-based SMT \citep{williams2016syntax}. The same trend is also found in neural net machine translation. For example, \citet{sennrich2016linguistic} exploit the information of lemmas, part of tags, morphology of words, and dependency parses of sentences to improve MT systems. \citet{ccg_target_seq} incorporate the Categorial grammar parse tags of the target sequences.

The Parallel-Partial treatment section \ref{sec:ParaPart} may be viewed as a form of multi-task Sequence to Sequence Learning \citep{luong2015multi}. Specifically, the parallel part of the treatment is very similar to the data manipulation used in building multi-language translation systems \citep{google_zero_shot}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\section{Technical Settings of the Machine Translation Experiments}\label{sec:experimet_setting}
The experiments are conduced by using OpenNMT \citep{2017opennmt}, which implements the state-of-the-art neural net machine translation algorithms \citep{cho2014properties, cho2014learning, bahdanau2014neural}.
The following default hyper-parameter settings of OpenNMT\footnote{See their documentation for the complete default hyper-parameter settings: \url{http://opennmt.net/OpenNMT-py/}.} are used across all models so that the only independent variable is the type of the training data:
	\begin{itemize}
	\item Word vector size: 500
	\item Type of recurrent cell: Long Short Term Memory
	\item Number of recurrent layers of the encoder and decoder: 2
	\item Number of epochs: 13
	\item Size of mini batches: 64\\
	\end{itemize}

The settings of the hyper-parameters do have effects on the performances of the trained models.
However, given that finding the optimal settings of the hyper-parameters is not relevant to our current research and causing unnecessary complications, the process of optimizing the settings of the hyper-parameters is not implemented, and we simply adopt OpenNMT's default settings. The employed settings of the hyper-parameters should be viewed as arbitrarily chosen options, and there are rooms to tune the models for better performance. We will leave the question of what hyper-parameters are optimal for our data for future research. Critically, these settings are viewed as constants, so that we can focus on the effects of different treatments on the source sequences in the translation experiments.

\section{Gloss Representation Solely Does NOT Outperform Gaelic Sentences} \label{gd_to_gl_to_en}
This section tests the strong version of Gloss-helps hypothesis in (\ref{strong_hy}).
Given the assumption that gloss may be better than any natural language in terms of representing meanings, it is expected that for neural net machine translation systems it is easier to learn how to translate from the glosses of Scottish Gaelic to English than to learn how to translate from Scottish Gaelic to English. However, the results show that there is no significance difference between the two types of data (i.e. GLOSS $\rightarrow$ English and Gaelic $\rightarrow$ English).

\subsection{Scottish Gaelic Interlinear Glossed Text Data}
We use the same Scottish Gaelic IGT corpus\footnote{Full citation cannot be given without compromising anonymity.} for all experiments. The corpus has 8,367 Gaelic sentences, and in term of words, it has 52,778 Gaelic words/glosses. The data of the corpus is from two different sources: linguistics fieldwork and data elicitation.

\subsection{Procedure of the Experiments}
We use repeated random sub-sampling validation to compare the performances of the two type of models.
Totally we have 8,388 indexed 3-tuples of a Gaelic sentence, a gloss line and an English translation. In the interlinear glossed text example below, each line is an argument of a 3-tuple sample.

\begin{exe} 
\ex \gll    Tha a athair nas sine na a mh\`athair.\\ 
           be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother
\\ 
   \glt    `His father is older than his mother.' 
\end{exe}

The 3-tuple representation of the above example is:
\begin{exe}
\ex <``Tha a athair nas sine na a mh\`athair .'', ``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``His father is older than his mother .''>
\end{exe}

First, the samples (i.e. the 3-tuples) are randomly split into three datasets: training set (N=6,388), validation set (N=1,000), and test set (N=1,000)\footnote{Here the random sampling process is achieved by using the \begin{myfont}random.sample(population, k)\end{myfont} function in the standard library of python.}.

For each index, the 3-tuple is split into two pairs: <gloss, English>, <Gaelic, English>, so that later we can compare the different effects of gloss lines and Gaelic sentences. 
\begin{exe}
	\ex Gloss to English
		\begin{xlist}
		\ex \label{GLOSStoENTrain} GLOSStoEN\textsubscript{Train}   =\\$\{<gloss_i,En_i>  \mid i \in Index\textsubscript{Train} \}$ \\
		\ex \label{GLOSStoENVal} GLOSStoEN\textsubscript{Validation}   = \\$\{<gloss_i,En_i>  \mid i \in Index\textsubscript{Validation} \}$ \\
		\ex \label{GLOSStoENTest}GLOSStoEN\textsubscript{Test} = \\$\{<gloss_i,En_i>  \mid i \in Index\textsubscript{Test} \}$ \\
		\ex  Example: <``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``His father is older than his mother .''>
		\end{xlist}

	
	\ex Gaelic to English
		\begin{xlist}
		\ex \label{GDtoENTrain} GDtoEN\textsubscript{Train}   = \\$\{<GD_i,En_i>  \mid i \in Index\textsubscript{Train} \}$ \\
		\ex \label{GDtoENVal} GDtoEN\textsubscript{Validation}   =\\ $\{<GD_i,En_i>  \mid i \in Index\textsubscript{Validation} \}$ \\
		\ex \label{GDtoENTest} GDtoEN\textsubscript{Test}    = \\$\{<GD_i,En_i>  \mid i \in Index\textsubscript{Test} \}$ \\
		\ex Example: <``Tha a athair nas sine na a mh\`athair .'', ``His father is older than his mother .''>
		\end{xlist}
\end{exe}
The models are trained with the training set and validation set. 

\begin{exe}
\ex Models:
	\begin{xlist}
	\ex \label{ModelGlossToEN} Model\textsubscript{GLOSStoEN} =\\ Model trained with GLOSStoEN\textsubscript{Train} in (\ref{GLOSStoENTrain}) and GLOSStoEN\textsubscript{Validation} in (\ref{GLOSStoENVal})
	\ex \label{ModelGDToEN}Model\textsubscript{GDtoEN} =\\ Model trained with GDtoEN\textsubscript{Train} in (\ref{GDtoENTrain}) and GDtoEN\textsubscript{Validation} in (\ref{GDtoENVal})
	\end{xlist}	
\end{exe}
The two trained models (gloss-to-English and Gaelic-to-English) then take the right source sequences of the test sets (i.e. glossing lines and Gaelic sentences for Model\textsubscript{GLOSStoEN} and Model\textsubscript{GDoEN} respectively) as inputs and then generate the predicted target sequences (i.e. English sentences).

To evaluate the model, the predicted target sequences are checked against the target sequences of the test set (i.e. the gold standard/human-translated English sentences).
Specifically, the BLEU score metric \citep{bleu} of each prediction is calculated using the \begin{myfont} multi-bleu.perl\end{myfont}\footnote{The script can be downloaded from: \url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}}
script, a public implementation of Moses \citep{moses}. 
\begin{exe}
\ex Gold-Standard = English sentences in (\ref{GLOSStoENTest}) = English sentences in (\ref{GDtoENTest})
\end{exe}
Note that the gold-standard is the same because they are the same English sentences in the 3-tuples samples. Then the two sets of predicted English sentences are evaluated, yielding two BLEU scores.  

\begin{exe}
\ex Scores: \\
 \begin{xlist}
	\ex Score\textsubscript{GLOSStoEN} = BLEU(Gold-Standard, Predictions\textsubscript{GLOSStoEN}) \\
	\ex Score\textsubscript{GDtoEN} = BLEU(Gold-Standard, Predictions\textsubscript{GDtoEN}) \\
 \end{xlist}
\end{exe}
This procedure of splitting the data into three sub-sets, training the models, and evaluating the models is executed for ten times. Recall that all the sets are randomly chosen samples. In this manner, we will be able to reduce the chance of sampling errors and do a t-test to compare the BLEU scores of the treatments.  

\subsection{Result} \label{gdglen_results}
After ten rounds of repeated random sub-sampling validation, ten pairs of scores of the two models are generated. See the appendix for the complete BLEU score for each models.
% !Rnw root = cake_chapter.Rnw

The average score of the Models\textsubscript{GLOSStoEN} is only sightly higher than the average score of the Models\textsubscript{GDtoEN}.
Also, a paired t-test shows that the difference between the two types of models is NOT statistically significant
(M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{GLOSStoEN}=17.70, SD\textsubscript{GLOSStoEN}=1.78; t(9)=1.97, p=0.080). 

\subsection{Discussion}
It is assumed that the performances of the machine translation systems are correlated with the quality of the representation of meanings in the source sequences. 
Better representations of meanings yield better machine translation systems. Given the results in (\ref{gdglen_results}) that the gloss models are not better than the Gaelic models, it is concluded that glosses and natural languages are equally good in terms of representing meanings. The strong version of the Gloss-helps hypothesis does not hold.

There are several remarks that need to make for the current result. First, the result falsifies the point of view about glosses that the gloss line is a golden semantic representation hand-crafted by linguists.
It turns that this artificial language, the gloss lines, is only marginal better than Gaelic statistically. This can be viewed as an evidence of language evolution.
The written form of a natural language is actually already optimized for representing semantics to the same degree as the gloss line representations.
Second, if we want to actually apply the gloss treatment to translate a Gaelic sentence to English, we encounter an immediate problem. The actual source sequence is a Gaelic sentence, while the required source sequence for the gloss treatment is a gloss line. Given this, even if the gloss treatment should work, it is not practical unless we may convert Gaelic sentences to gloss lines perfectly.      

We may now combine Gaelic sentences and gloss lines as the training data to test the weak version of the Gloss-helps hypothesis. The experiments and results are reported in the next section.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Right Way of Combining Gaelic Words with Glosses: Parallel-Partial Treatment}\label{gd_plus_gl_to_en}
In the previous section, we attempt to build systems by using the \textit{gloss treatment} to outperform the baseline system. It turns that using gloss line solely is not effective enough to improve the system. However, this result does not falsify the gloss-helps hypothesis; instead, it indicates that combination of the gloss line data and the Gaelic sentence data is necessary. In other words, the questions now are: 
\begin{exe}
	\ex 
	\begin{xlist}
		\ex Will adding the gloss data into the Gaelic data improve the translation system? 
		\ex If yes, what are the right ways of blending these two types of meaning representations together? 
	\end{xlist}	
\end{exe}

This section reports that a specific way of combining Gloss data and Gaelic date (termed as `\textit{Parallel-Partial}' treatment) boosts the performance significantly. The models trained with this specially arranged training data also significantly outperform Google's Gaelic-to-English translation system\footnote{ We used a free Google translation API \citep{google_api} to translate the Gaelic sentences in our test set. Then we calculate the BLEU scores of with the target sequences of our test set as the gold standard.} (see the appendix for the BLEU scores).

\subsection{The Underlying Heuristics}\label{heuristics}
At a high level, neural net sequence to sequence learning algorithm is to learn how to map a high-dimension space to another high-dimension space. In the settings of machine translation, each dot in the high-dimension space is a meaning representation. Linking one dot to another dot is converting one meaning representation to another, yielding the effect of translation. Given this heuristics, we may just feed the machine with all the available meaning mappings. Given the assumption that the gloss lines are linguistically guided meaning representations, they are suitable training data for building machine translation systems. Specially, with the gloss data, we let the machine to learn the following mappings:

\begin{exe}
	\ex Mappings Learned in the ParaPart treatment
	\begin{xlist}
		\ex Gaelic sentences $\rightarrow$ English sentences
		\ex Gloss lines $\rightarrow$ English sentences
		\ex Gloss lines $\rightarrow$ Gaelic sentences
		\ex Gaelic words $\rightarrow$ Gloss items
	\end{xlist}	
\end{exe}    

\subsection{The `Parallel-Partial' Treatment Outperforms the Baseline Significantly}\label{sec:ParaPart}

\subsubsection{Data Preprocessing Using the Parallel-Partial Treatment}
The Parallel-Partial treatment uses the training and validation data of the baseline system and that of the gloss treatment system.  
The training and validation data of the baseline system are pairs of a Gaelic sentence and a English sentences (see (\ref{GDtoENTrain}) and (\ref{GDtoENVal}) ), 
and the data of the gloss treatment are pairs of a gloss line and a English sentences (see (\ref{GLOSStoENTrain}) and (\ref{GLOSStoENVal}). 
These two groups of data are combined in a parallel manner in the current treatment. Additionally, we also fed the machine with mappings from glossing lines to Gaelic sentences. Now the size of training set and validation set is tripled. In the baseline system and the gloss treatment system, we have 6,388 samples in the training set and 1,000 samples in the validation set. The current treatment has 19,164 (6,388*3) samples in the training set and 3,000 (1,000*3) samples in the validation set. This is the \textit{parallel} part of the treatment. 

Additionally, we utilize the alignment property between the Gaelic word and the gloss to further build pairs of a Gaelic word and a gloss. These pairs are also included into the training set and validation set of the current treatment. This is the \textit{partial} part of the treatment. 

For concreteness, consider the following interlinear glossed text: 
\begin{exe}  
\ex \gll    Tha a athair nas sine na a mh\`athair.\\  
            be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother\\  
    \glt    `His father is older than his mother.'  
\end{exe}

With the interlinear glossed text, the parallel treatment will generate two pairs of samples:

\begin{exe}
	\ex \label{21}
	\begin{xlist}
		\ex Gaelic to English: \\<``Tha a athair nas sine na a mh\`athair'', ``His father is older than his mother.''>
		\ex Gloss to English: \\<``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``His father is older than his mother''>
		\ex Glosses to Gaelic: \\<``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``Tha a athair nas sine na a mh\`athair''>
	\end{xlist}
\end{exe}

The partial treatment then generates pairs of a Gaelic word and a gloss token: 
\begin{exe}
	\ex\label{22}
	\begin{xlist}
		\ex <``Tha'', ``be.pres''>
		\ex <``a'', ``3sm.poss''>
		\ex <``athair'', ``father''>
		\ex <``nas'', ``comp''>
		\ex <``sine'', ``old.cmpr''>
		\ex <``na'', ``comp''>
		\ex <``a'', ``3sm.poss''>
		\ex <``mh\`athair'', ``mother''>
	\end{xlist}
\end{exe}

The samples like (\ref{21}) and (\ref{22}) are the training data for the Parallel-Partial treatment. 

\subsubsection{Results of the Parallel-Partial Treatment}
% !Rnw root = cake_chapter.Rnw
The same technical settings and the same test sets in the previous experiments are used, and the same procedures are executed. 
The only difference is the training and validation data. The result show that the Parallel-Partial treatment has a tremendous effect in improving the baseline system (M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{ParaPart}=32.10, SD\textsubscript{ParaPart}=1.33; t(9)=48.95, p<0.01). 

\subsubsection{Discussion}
With the ParaPart treatment, the baseline systems are improved for more than 93 percent. This result suggest the validity of our heuristics in section \ref{heuristics}, and provide strong evidence for the gloss-helps hypothesis in (\ref{hypothesis}).      





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other Possible Treatments}
There other possible ways of blending the Gaelic sentences and gloss lines. However, all of these treatments are not as effective as the Parallel-Partial treatment. Again, the same procedure and the same test datasets are used across all the experiments.    

\subsection{The Parallel Treatment}\label{treatment:Para}
\subsubsection{Method of the Parallel Treatment}
The Parallel treatment is using the parallel part of the Parallel-Partial treatment without exploiting the alignment properties of gloss lines.
It is expected that this treatment will improve the baseline systems but will not be as effective as the Parallel-Partial treatment.

With this treatment, a chunk of interlinear glossed text is split into three pairs. For example, the chunk of interlinear glossed text in (\ref{igt}) becomes three samples in (\ref{sample_pair}): 
\begin{exe} 
\ex \label{igt}
	\gll    Tha a athair nas sine na a mh\`athair.\\  
            be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother \\
    \glt    `His father is older than his mother.'  
\end{exe}


\begin{exe} 
	\ex \label{sample_pair}
	\begin{xlist}
		\ex Gaelic to English: \\<``Tha a athair nas sine na a mh\`athair'', ``His father is older than his mother.''>
		\ex Gloss to English: \\<``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``His father is older than his mother''>
		\ex Gloss to Gaelic: \\<``be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother'', ``Tha a athair nas sine na a mh\`athair''>
	\end{xlist}
\end{exe}

\subsubsection{Results of the Parallel Treatment}\label{treatment:Para_result}
% !Rnw root = cake_chapter.Rnw
The experiments had our expected results.  
The Parallel treatment is effective in improving the baseline systems (M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{Para}=29.56, SD\textsubscript{Para}=1.46; t(9)=34.42, p < 0.01). 
% !Rnw root = cake_chapter.Rnw
%GDParaParaPart1_table.Rnw does NOT print out anything but just load the sexpr variables 
However, the best treatment (i.e. the Parallel-Partial treatment) is still far better than this Parallel treatment 
(M\textsubscript{Para}=29.56, SD\textsubscript{Para}=1.46; M\textsubscript{ParaPart}=32.10, SD\textsubscript{ParaPart}=1.33; t(9)=8.76, p < 0.01 ).
% !Rnw root = cake_chapter.Rnw

Critically, the comparison between the Parallel-Partial treatment and current Parallel-Only treatment shows the effectiveness of the word-gloss alignments. Our conjecture on the effectiveness is that with the pairs of a gloss item and a Gaelic word present in the training data, the burden of the attention algorithm \citep{bahdanau2014neural} is largely alleviated. In other words, instead of asking the attention algorithm to estimate what to attend to, we explicitly teach the machine the alignment between the Gaelic word and the corresponding gloss. 

\subsection{Interleaving Gaelic Words and Gloss Items And Concating them}\label{treatment:InterleavingAndConCat}
\subsubsection{Method of the Interleaving Treatment}
Instead of putting the pairs of a Gaelic sentence and a English sentences and the pairs of a gloss line and a English sentence in a parallel manner, we may just literally blend a Gaelic sentence and a gloss line by interleaving them\footnote{\citet{ccg_target_seq} incorporate the Categorial grammar parse tags into natural sentences by interleaving the tags and the words.}. Consider the following example:

\begin{exe} 
\ex 
	\begin{xlist}
	\ex \label{ex_interleave:in}
		\gll	 Tha a athair nas sine na a mh\`athair.\\  
     		     be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother \\
    	\glt    `His father is older than his mother.'  

    \ex \label{ex_interleave:out} <``Tha be.pres a 3sm.poss athair father nas comp sine old.cmpr na comp a 3sm.poss mh\`athair mother'', ``His father is older than his mother''>
    \end{xlist}
\end{exe}

Given the chuck of interlinear glossed text data in (\ref{ex_interleave:in}), the Interleaving treatment generates the sample in (\ref{ex_interleave:out}).  

This way of blending gloss lines and Gaelic sentences may add useful information into the training data; however, the downside of this method is to increase the length our samples on the source sequence side. In neural net machine learning, the longer the sequences are, the harder it is to preserve all the information. So, this treatment may not be effective. 

% !Rnw root = cake_chapter.Rnw

It turns out this treatment has a significant negative effect
(M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{interleavingGdGLOSS}=12.26, SD\textsubscript{interleavingGdGLOSS}=0.74,; t(9)=-17.06, p=0.000). This is not the right way of incorporating gloss line data. 


\subsubsection{Method of Concating Gaelic Words and Gloss Words }\label{treatment:Concating}
A quick and close amendment of the Interleaving approach is to concatenate the aligned Gaelic word and gloss item as a single token. Given the same chunk of interlinear glossed text data, this treatment generates the following sample:

\begin{exe} 
\ex 
	\begin{xlist}
	\ex 
		\gll	 Tha a athair nas sine na a mh\`athair.\\  
     		     be.pres 3sm.poss father comp old.cmpr comp 3sm.poss mother \\
    	\glt    `His father is older than his mother.'  

    \ex <``Tha\_be.pres a\_3sm.poss athair\_father nas\_comp sine\_old.cmpr na\_comp a\_3sm.poss mh\`athair\_mother'', ``His father is older than his mother''>
    \end{xlist}
\end{exe}

Concating words and glosses does solve the long sequence problem; however, it causes the sparse data problem. In this arrangement, the number of the types of tokens is increased; the number of tokens of each type is decreased. Thus all the samples are put in a larger space. So, the treatment may not be effective either. 

\subsubsection{Results of Concating Gaelic Words and Gloss Words}
% !Rnw root = cake_chapter.Rnw

The result shows that this treatment hurts the baseline systems instead of improving them (M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{ConcatGLOSSGaelic}=15.44, SD\textsubscript{ConcatGLOSSGaelic}=1.23,; t(9)=-3.64, p=0.010).

%%%%%%%%%%
%!!! Not going to talk about Hybrid methord 
% \subsection{Hybrid: Gaelic or Gloss}
% \subsubsection{Method of Hybrid}
% The Hybrid treatment aims to reduce the potential lexical ambiguity. A Gaelic word may maps to multiple gloss, and a glosses may maps to multiple Gaelic words. Let's assume a toy chunk of interlinear glossed text data (a one-word sentence): 

% \begin{exe} 
% \ex 
% 	\gll	 Gaelic\_word\\  
%      		 Gloss\_item \\
%     \glt    English translation  
% \end{exe} 

% Now we aim to build a single sample that is either <Gaelic\_word, English translation > or <Gloss\_item, English translation >. The criterion is which one, the Gaelic word or the gloss item, is less ambiguous. The less ambiguous one is the winner. For example, if the Gaelic word is potentially mapped to 10 glosses and if the gloss item is potentially mapped 2 Gaelic word, then <Gloss\_item, English translation> is chosen; other the other hand if the ambiguity situation is reverse, then <Gaelic\_word, English translation > is chosen. However, when the situation is tight (i.e. both the Gaelic word and gloss item are equally ambiguous), a default setting is needed to be chosen. The choices of the default setting split this single treatment into two treatments: default as Gaelic word or default as gloss.

% \subsubsection{Result of Hybrid}    
% \SweaveInput{alcReplacingGaelic_table.Rnw}
% When the default setting is the Gaelic word, the performances are significantly worse than than the baseline systems
% (M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{ReplacingGaelic}=15.44, SD\textsubscript{ReplacingGaelic}=1.23,; t(9)=-3.64, p < 0.01).


% \SweaveInput{alcReplacingGLOSS_table.Rnw}
% When the default setting is the Gaelic word, the performances are sightly worse than than the baseline systems
% (M\textsubscript{GDToEn}=16.59, SD\textsubscript{GDToEn}=0.74; M\textsubscript{ReplacingGaelic}=15.44, SD\textsubscript{ReplacingGaelic}=1.23,; t(9)=-3.64, p < 0.01 ).


% \section{Summary and Conclusion}
% The chapter reports machine translation experiments that aims to find how the gloss line information can improve the performance of the baseline Gaelic-to-English translation systems. It is found that the Parallel-Partial is highly effective. 
\section{Conclusion}
In the current paper, we introduce an very effective way of incorporating the gloss data into neural net machine translation systems. The immediate merit is that it works. Additional, how theoretical linguistics may work hand in hand with natural language processing, and how neural net machine learning may exploit linguistics are important questions in both fields (see \citet{pater2017generative} for a nice discussion on this topic). In addition to practically building better MT systems, the current work also exemplifies how theoretical linguistics may work hand in hand with natural language processing successfully. 


\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\appendix
\newpage
\section{Supplemental Material}
% latex table generated in R 3.4.4 by xtable 1.8-2 package
% Mon Apr 16 10:44:45 2018
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrrr}
  \hline
Round & Baseline & GLOSS & ParaPart & Para & Interleaving & Concat & Google Translation \\ 
  \hline
0 & 17.29 & 18.39 & 32.64 & 25.42 & 13.67 & 15.42 & 22.09 \\ 
  1 & 16.42 & 18.00 & 32.28 & 25.32 & 12.49 & 14.31 & 25.38 \\ 
  2 & 15.29 & 16.02 & 29.94 & 20.72 & 11.01 & 15.38 & 23.72 \\ 
  3 & 15.97 & 20.22 & 31.18 & 22.22 & 12.33 & 14.18 & 23.21 \\ 
  4 & 17.79 & 19.02 & 32.83 & 24.27 & 12.56 & 18.63 & 22.31 \\ 
  5 & 16.73 & 15.53 & 31.11 & 24.55 & 12.13 & 14.89 & 23.41 \\ 
  6 & 17.11 & 18.00 & 32.19 & 27.03 & 11.55 & 15.16 & 24.53 \\ 
  7 & 16.37 & 20.08 & 33.52 & 25.34 & 12.78 & 15.20 & 22.78 \\ 
  8 & 15.93 & 15.82 & 30.93 & 24.24 & 12.43 & 15.50 & 25.67 \\ 
  9 & 16.99 & 15.93 & 34.35 & 25.96 & 11.65 & 15.72 & 23.42 \\ 
   \hline
Mean & 16.59 & 17.70 & 32.10 & 24.51 & 12.26 & 15.44 & 23.65 \\ 
   \hline
\end{tabular}
\caption{BLEU scores of the treatments: Ten rounds of repeated random sub-sampling validation are conducted. For each round, the same sets of IGTs are used. Each column is a treatment, and each row is a single round of repeated random sub-sampling validation. The last column is the scores of Google Translation. We used a free Google translation API \citep{google_api} to translate the same set of test Gaelic sentences into English.} 
\label{table:complete_table}
\end{table}
\end{document}
